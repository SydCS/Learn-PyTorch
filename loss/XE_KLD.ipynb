{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross Entropy Loss\n",
        "\n",
        "本notebook演示了交叉熵损失的三种等价计算方式：\n",
        "1. 使用PyTorch的CrossEntropyLoss\n",
        "2. 使用LogSoftmax + NLLLoss组合\n",
        "3. 手动实现交叉熵计算过程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备示例数据\n",
        "num_classes = 3\n",
        "# 模型输出的原始logits\n",
        "raw_predictions = torch.tensor([[1.2, 0.5, -0.3],  # 预测样本1\n",
        "                              [-0.1, 2.0, -0.5],   # 预测样本2\n",
        "                              [0.3, -0.2, 1.5]])   # 预测样本3\n",
        "# 真实标签\n",
        "targets = torch.tensor([0, 1, 2])  # 分别对应类别0, 1, 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 方法1：直接使用CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "方法1 - CrossEntropyLoss: 0.374305\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "loss1 = criterion(raw_predictions, targets)\n",
        "print(f\"方法1 - CrossEntropyLoss: {loss1.item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 方法2：LogSoftmax + NLLLoss组合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "方法2 - LogSoftmax + NLLLoss: 0.374305\n"
          ]
        }
      ],
      "source": [
        "log_softmax = nn.LogSoftmax(dim=1)\n",
        "nll_loss = nn.NLLLoss()\n",
        "\n",
        "# 计算 log_softmax\n",
        "log_probs = log_softmax(raw_predictions)\n",
        "# 计算 negative log likelihood loss\n",
        "loss2 = nll_loss(log_probs, targets)\n",
        "print(f\"方法2 - LogSoftmax + NLLLoss: {loss2.item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 方法3：手动实现交叉熵计算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "方法3 - 手动实现: 0.374305\n"
          ]
        }
      ],
      "source": [
        "# 创建one-hot编码的目标分布\n",
        "target_dist = torch.eye(num_classes)[targets]\n",
        "\n",
        "# 手动计算交叉熵：-sum(target_dist * log_probs)\n",
        "loss3 = torch.mean(torch.sum(-target_dist * log_probs, dim=1))\n",
        "print(f\"方法3 - 手动实现: {loss3.item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 验证三种方法结果相同\n",
        "assert torch.allclose(loss1, loss2) and torch.allclose(loss2, loss3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KL Divergence\n",
        "\n",
        "KL散度用于衡量两个概率分布之间的差异。\n",
        "- P: 真实分布\n",
        "- Q: 预测分布\n",
        "- KL(P||Q) = Σ P(x) * log(P(x)/Q(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "# input should be a distribution in the log space\n",
        "input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
        "# Sample a batch of distributions. Usually this would come from the dataset\n",
        "target = F.softmax(torch.rand(3, 5), dim=1)\n",
        "output = kl_loss(input, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备示例数据：两个概率分布\n",
        "P = torch.tensor([[0.9000, 0.1000, 0.0000],\n",
        "\t\t\t\t  [0.2000, 0.7000, 0.1000],\n",
        "\t\t\t\t  [0.1000, 0.2000, 0.7000]])\n",
        "Q = torch.tensor([[0.8000, 0.1500, 0.0500],\n",
        "\t\t\t\t  [0.2500, 0.6000, 0.1500],\n",
        "\t\t\t\t  [0.1500, 0.2500, 0.6000]])\n",
        "\n",
        "assert torch.allclose(P.sum(dim=1), torch.ones(3))\n",
        "assert torch.allclose(Q.sum(dim=1), torch.ones(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 方法1：使用PyTorch的KLDivLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch KLDivLoss: 0.036973\n"
          ]
        }
      ],
      "source": [
        "kl_div = nn.KLDivLoss(reduction='batchmean')\n",
        "# 注意：KLDivLoss期望输入是log概率\n",
        "loss_kl = kl_div(torch.log(Q), P)\n",
        "print(f\"PyTorch KLDivLoss: {loss_kl.item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 方法2：手动实现KL散度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "手动实现 KL 散度: 0.036973\n"
          ]
        }
      ],
      "source": [
        "# 添加一个很小的值来避免log(0)\n",
        "epsilon = 1e-15\n",
        "P = P + epsilon\n",
        "Q = Q + epsilon\n",
        "\n",
        "# 重新归一化，确保概率和为1\n",
        "P = P / P.sum(dim=1, keepdim=True)\n",
        "Q = Q / Q.sum(dim=1, keepdim=True)\n",
        "\n",
        "# 手动计算 KL(P||Q) = Σ P(x) * log(P(x)/Q(x))\n",
        "kl_div_manual = torch.mean(torch.sum(P * (torch.log(P) - torch.log(Q)), dim=1))\n",
        "print(f\"手动实现 KL 散度: {kl_div_manual.item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 验证两种方法结果相同\n",
        "assert torch.allclose(loss_kl, kl_div_manual, rtol=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KL散度的特性示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KL(P||Q): 0.036973\n",
            "KL(Q||P): 0.530659\n",
            "注意：KL(P||Q) ≠ KL(Q||P)，说明KL散度是不对称的\n"
          ]
        }
      ],
      "source": [
        "# 演示KL散度的不对称性\n",
        "kl_pq = torch.mean(torch.sum(P * (torch.log(P) - torch.log(Q)), dim=1))\n",
        "kl_qp = torch.mean(torch.sum(Q * (torch.log(Q) - torch.log(P)), dim=1))\n",
        "\n",
        "print(f\"KL(P||Q): {kl_pq.item():.6f}\")\n",
        "print(f\"KL(Q||P): {kl_qp.item():.6f}\")\n",
        "print(\"注意：KL(P||Q) ≠ KL(Q||P)，说明KL散度是不对称的\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Distillation Loss: 1.6067055463790894\n"
          ]
        }
      ],
      "source": [
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=2.0, alpha=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.kl_div = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, targets):\n",
        "        # Hard Loss: student predictions with ground truth\n",
        "        hard_loss = self.criterion(student_logits, targets)\n",
        "\n",
        "        # Soft Loss: distillation with teacher predictions\n",
        "        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)\n",
        "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        soft_loss = self.kl_div(soft_student, soft_teacher) * (self.temperature**2)\n",
        "\n",
        "        # Combined loss\n",
        "        loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
        "        return loss\n",
        "\n",
        "\n",
        "# 示例使用\n",
        "# Assuming we have a batch of 3 samples with 10 classes each\n",
        "batch_size, num_classes = 3, 10\n",
        "\n",
        "# Randomly generated logits for demonstration purposes\n",
        "student_logits = torch.randn(batch_size, num_classes)\n",
        "teacher_logits = torch.randn(batch_size, num_classes)\n",
        "true_labels = torch.randint(0, num_classes, (batch_size,))\n",
        "\n",
        "# Initialize the distillation loss with a specified temperature and alpha\n",
        "criterion = DistillationLoss(temperature=3.0, alpha=0.7)\n",
        "\n",
        "# Compute the loss\n",
        "loss = criterion(student_logits, teacher_logits, true_labels)\n",
        "print(f\"Total Distillation Loss: {loss.item()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
